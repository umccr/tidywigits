---
title: "Structure"
vignette: >
  %\VignetteIndexEntry{Structure}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
knitr:
  opts_chunk:
    collapse: true
    comment: '#>'
---

```{r}
#| label: pkgs
#| message: false
#| warning: false
#| echo: false

{
  require(tidywigits)
  require(dplyr)
  require(glue, include.only = "glue")
  require(here, include.only = "here")
  require(purrr, include.only = "map")
  require(readr, include.only = "read_tsv")
  require(tibble, include.only = "tibble")
  require(tidyr, include.only = "unnest")
}
```

{tidywigits} is an R package that parses and tidies outputs from the [WiGiTS]
suite of genome and transcriptome analysis tools for cancer research and diagnostics,
created by the Hartwig Medical Foundation.

In short, it traverses through a directory containing results from one or more
runs of WiGiTS tools, parses any files it recognises (in-memory, yes), tidies
them up (which includes data reshaping, normalisation, column name cleanup etc.),
and writes them to the output format of choice e.g. Apache Parquet, PostgreSQL,
Delta table.

[WiGiTS]: <https://github.com/hartwigmedical/hmftools> "WiGiTS suite"

{tidywigits} is built on top of R's [R6] encapsulated object-oriented programming
implementation, which helps with code organisation. It consists of several base
classes like `File`, `Config`, `Tool`, and `Workflow` which we describe below.
Each R6 class can contain public and private functions and non-functions (fields).

[R6]: <https://github.com/r-lib/R6> "R6 package repo"


## `File`

A `File` object represents... a file. It has an absolute path, a basename, and
a size. Given a suffix pattern, it can spit out its prefix.

```{r}
#| label: file_example1
(f1 <- readr::readr_example("mtcars.csv"))
(f1_obj <- File$new(path = f1, suffix_pattern = "\\.csv"))
```

You can access the individual fields in the classic R list-like manner, using
the `$` sign:

```{r}
#| label: file_example2
f1_obj$suffix_pattern
f1_obj$prefix
```

## `Config`

A `Config` object contains functionality for interacting with YAML
configuration files that are part of {tidywigits}. These configuration files
(under `inst/config`) specify the schemas, types, patterns and field descriptions
for the _raw_ input _files_ and _tidy_ output _tbls_.

### raw

Let's look at some of the information for the raw PURPLE config, for instance:

```{r}
#| label: purple_config1
tool <- "purple"
toolu <- toupper(tool)
conf <- Config$new(tool)
```

::: {.panel-tabset}

#### descriptions

File descriptions based on the Hartwig documentation.

```{r}
#| label: purple_config_raw_descriptions
conf$get_raw_descriptions() |>
  knitr::kable(caption = glue("{toolu} raw file descriptions."))
```

#### patterns

Patterns are used to fish out the relevant files from a directory listing.
Note that the `\` needs to be doubled in the R code since it's an escaped
character.

```{r}
#| label: purple_config_raw_patterns
conf$get_raw_patterns() |>
  knitr::kable(caption = glue("{toolu} raw file patterns."))
```

#### versions

Versions are used to distinguish changes in schema between individual tool
versions. For example, after LINX v1.25, several columns were
dropped from the `breakends` table, which is reflected in the available LINX
schemas. For now we are using `latest` as a default version based on the most
recent schema tests, and any discrepancies we see are labelled accordingly by the
version of the tool that generated a file with a different schema.

```{r}
#| label: purple_config_raw_versions
conf$get_raw_versions() |>
  knitr::kable(caption = glue("{toolu} raw file versions."))
```

:::

#### schemas

The raw schemas specify the column name and type (e.g. character (`c`),
integer (`i`), float/double (`d`))
for each input file (just showing top few below):

::: {.panel-tabset}

```{r}
#| label: purple_config_raw_schemas1
#| results: asis
s <- conf$get_raw_schemas_all()
for (nm in s[["name"]]) {
  cat(glue("##### {nm}  \n\n"))
  cat("\n\n")
  # TODO: account for non-latest versions
  s |>
    dplyr::filter(.data$name == nm) |>
    dplyr::select("schema") |>
    tidyr::unnest("schema") |>
    dplyr::slice_head(n = 5) |>
    knitr::kable() |>
    print()
  cat('\n\n')
}
```

:::

### tidy

Now let's look at some of the information in the tidy PURPLE config. The
difference between raw and tidy configs is mostly in the column names (they are
standardised to lowercase separated by underscores, i.e. snake_case), and some
raw files get split into multiple tidy tables (e.g. for normalisation purposes).

::: {.panel-tabset}

#### descriptions

Tidy descriptions are the same as the raw descriptions for now.

```{r}
#| label: purple_config_tidy_descriptions
conf$get_tidy_descriptions() |>
  knitr::kable(caption = glue("{toolu} tidy file descriptions."))
```

:::

#### schemas

::: {.panel-tabset}

```{r}
#| label: purple_config_tidy_schemas1
#| results: asis
s <- conf$get_tidy_schemas_all()
for (nm in s[["name"]]) {
  cat(glue("##### {nm}  \n\n"))
  cat("\n\n")
  # TODO: account for non-latest versions + diff tbl
  s |>
    dplyr::filter(.data$name == nm) |>
    dplyr::select("schema") |>
    tidyr::unnest("schema") |>
    dplyr::slice_head(n = 5) |>
    knitr::kable() |>
    print()
  cat('\n\n')
}
```

:::

## `Tool`

`Tool` is the main organisation class for all file parsers and tidiers.
It contains functions for parsing and tidying typical CSV/TSV files (with
column names), and TXT files where the column names are missing. Currently it
utilises the very simple `readr::read_delim` function from the {[readr]} package
that reads all the data into memory.
These simple parsers are used in 80-90% of cases, so in the future we can
optimise the parsing if needed with faster packages such as {[data.table]},
{[duckdb-r]} or {[neo-r-polars]}.

[readr]: <https://github.com/tidyverse/readr> "readr"
[duckdb-r]: <https://github.com/duckdb/duckdb-r> "duckdb-r"
[data.table]: <https://github.com/Rdatatable/data.table> "data.table"
[neo-r-polars]: <https://github.com/eitsupi/neo-r-polars> "neo-r-polars"

### initialise

We can have different `Tool` children classes that inherit (or override) functions
and fields from the `Tool` parent class.
For example, we can create a `Tool` object for PURPLE as follows:

- Initialise a `Purple` object:

```{r}
#| label: tool_purple
ppl_path <- here::here("nogit/oa_v1/purple")
ppl <- Purple$new(path = ppl_path)
# each class comes with a print function
ppl
```

### config

- Its `Config` object is also constructed based on the `name` supplied - this
is used internally to find files of interest and infer their schemas:

```{r}
#| label: ppl_conf
ppl$config
ppl$config$get_raw_patterns()
ppl$config$get_raw_schema("puritytsv")
ppl$config$get_tidy_schema("puritytsv")
```

### list

We can list files that can be parsed with `list_files`:

```{r}
#| label: ppl_list
(lf <- ppl$list_files())
lf |> dplyr::slice(1) |> str()
```

### tidy

We can parse and tidy files of interest using the `tidy` function. Note
that this function is called on the object and not assigned anywhere:

```{r}
#| label: ppl_tidy
# this will create a new field tbls containing the tidy data (and optionally
# the 'raw' parsed data)
ppl$tidy(tidy = TRUE, keep_raw = TRUE)
ppl$tbls
ppl$tbls$raw[[8]] |> dplyr::glimpse()
# the tidy tibbles are nested to allow for more than one tidy tibble per file
ppl$tbls$tidy[[8]][["data"]][[1]] |> dplyr::glimpse()
```

### filter

We can also focus on a subset of files to tidy using the `filter_files` function.
The `include` and `exclude` arguments can specify which parsers to include or
exclude in the analysis:

```{r}
#| label: ppl_filter
# create new Purple object
ppl2 <- Purple$new(path = ppl_path)
ppl2$files
ppl2$filter_files(include = c("purple_qc", "purple_cnvsomtsv"))
ppl2$files
```

### write

After tidying the data of interest, we can write the tidy tibbles to various
formats, like Apache Parquet, PostgreSQL, CSV/TSV and R's RDS.
Below we can see that the `id` specified is added to the written files in
an additional `nemo_id` column. This can be used e.g. to distinguish results
from different runs in a data pipeline.
When writing to a database like PostgreSQL, another column `nemo_pfix` is used
to distinguish results from the same run from the same tool.

```{r}
#| label: ppl_write
ppl2$tidy() # first need to tidy
outdir1 <- tempdir()
fmt <- "tsv"
ppl2$write(odir = outdir1, format = fmt, id = "run123")
(wfiles <- fs::dir_info(outdir1) |> dplyr::select(1:5))
readr::read_tsv(wfiles$path[2], show_col_types = F)
```

### nemofy

The `nemofy` function is a convenient wrapper for the process of filtering,
tidying, and writing.

```{r}
#| label: nemofy1
ppl3 <- Purple$new(path = ppl_path)
outdir2 <- file.path(tempdir(), "ppl3") |> fs::dir_create()
ppl3$files
ppl3$nemofy(
  odir = outdir2,
  format = "tsv",
  id = "run_ppl3",
  exclude = c("purple_cnvgenetsv", "purple_cnvsomtsv", "purple_drivercatalog", "purple_germdeltsv")
)
(wfiles2 <- fs::dir_info(outdir2) |> dplyr::select(1:5))
readr::read_tsv(wfiles2$path[2], show_col_types = F)
```

## `Workflow`

A `Workflow` consists of a list of one or more `Tool`s. We can construct a
certain `Workflow` with different `Tool`s, which would allow parsing and writing
tidy tables from a variety of bioinformatic tools.
The [Oncoanalyser] Nextflow pipeline uses several tools from [WiGiTS], and we
can construct a `Oncoanalyser` class as a `Workflow` child based on a suite of
`Tool`s under the [tidywigits] R package. Similarly to `Tool`, a `Workflow` object
contains functions such as `filter_files`, `list_files`, `tidy`, `write` and
`nemofy`:

[Oncoanalyser]: <https://github.com/nf-core/oncoanalyser> "Oncoanalyser"
[tidywigits]: <https://github.com/umccr/tidywigits> "tidywigits"


```{r}
#| label: oa_init
oa <- Oncoanalyser$new(here::here("nogit/oa_v1"))
outdir3 <- file.path(tempdir(), "oa") |> fs::dir_create()
oa$list_files()
x <- oa$nemofy(
  odir = outdir3,
  format = "tsv",
  id = "oa_run1",
  exclude = c("cobalt_ratiotsv", "amber_baftsv", "isofox_altsj", "isofox_transdata")
)
(wfiles3 <- fs::dir_info(outdir3) |> dplyr::select(1:5))
readr::read_tsv(wfiles3$path[5], show_col_types = F)
```